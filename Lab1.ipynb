{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f70681",
   "metadata": {},
   "source": [
    "# Архитектура информационной системы ColorAIze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd510d",
   "metadata": {},
   "source": [
    "РЕФЕРАТ\n",
    "\n",
    "Архитектура информационной системы колоризации изображений на основе модели DeOldify\n",
    "\n",
    "Содержание\n",
    "\n",
    "1. Введение\n",
    "\n",
    "Методы искусственного интеллекта применяются для восстановления и обработки изображений, включая задачу колоризации. По данным MarketsandMarkets (2024), сегмент технологий реконструкции изображений составляет около 22 % рынка компьютерного зрения, что подтверждает устойчивый интерес к подобным решениям. Колоризация исторических и черно-белых фотографий востребована в музеях, архивах и медиапроектах, где требуется визуальное восстановление и цифровая реконструкция материалов. Актуальность темы обусловлена ростом числа цифровых архивов и потребностью в автоматизации обработки визуальных данных.\n",
    "\n",
    "Распределение сегментов рынка технологий компьютерного зрения в 2024 году\n",
    "\n",
    " Цель проекта — разработка информационной системы, выполняющей автоматическую колоризацию загруженных пользователем изображений с применением нейронной сети DeOldify.\n",
    "\n",
    "Почему именно нейронная сеть (НС) выбрана для задачи колоризации изображений\n",
    "\n",
    "1.1 Природа задачи и её сложность\n",
    "\n",
    "Задача автоматической колоризации — это ill-posed inverse problem (много возможных корректных раскрасок для одного чёрно-белого изображения) — как отмечено в исследовании “A review of image and video colorization: From analogies to deep …” .\n",
    "\n",
    "Нейронные сети способны моделировать сложные нелинейные зависимости между признаками изображения (градиенты, формы, текстуры, семантика) и цветовыми каналами, что классическим алгоритмам крайне сложно. Например, CNN-архитектуры используют автоматическое извлечение признаков, без ручного кодирования фильтров.\n",
    "\n",
    "В отчёте CS231n “Let There Be Color: Deep Learning Image Colorization” продемонстрировано, что нейронная сеть обучилась «понимать» семантику сцены (без меток) и использовать её для цветизации. \n",
    "\n",
    "Рисунок 1Схема процесса колоризации изображений с использованием модели DeOldify\n",
    "\n",
    "1.2 Эффективность НС по сравнению с классическими методами\n",
    "\n",
    "В обзоре Anwar et al. (2020) “Image Colorization: A Survey and Dataset” подчёркивается, что методы на основе глубокого обучения значительно продвинулись по качеству автоматической цветизации по сравнению с классическими методами переноса цвета или ручной сегментации+раскраски.\n",
    "\n",
    "В статье “A Comparative Study of Different Models on Image Colorization …” (2023) указано, что использование глубоких нейронных сетей (CNN) и transfer learning позволяет повысить качество и снизить время обработки по сравнению с ручными/гибридными методиками.\n",
    "\n",
    "1.3 Почему генеративные модели (НС-GAN/NoGAN) предпочтительны\n",
    "\n",
    "Цветизация требует не просто точного совпадения по пикселям, но «естественности» и восприятия человеком. В таких задачах глубинные генеративные модели (GAN-основанные) показывают лучшие результаты. Например исследование “ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution” (2019) показывает, что GAN-подход с семантической информацией улучшает реализм цвета.\n",
    "\n",
    "Обзорная статья “Computer-aided Colorization State-of-the-science: A Survey” (2024) отмечает, что нейронные сети с GAN или U-Net архитектурами обеспечивают лучшую визуальную реалистичность и скорость, чем классические методы.\n",
    "\n",
    "1.4 Почему НС позволяют масштабировать и адаптировать решение\n",
    "\n",
    "Нейронные сети легко адаптируются через transfer learning/fine-tuning: предобученная модель может быть дообучена на новом датасете, что значительно экономит ресурсы и улучшает результаты. Исследование Entezari (2023) «The Role of Pre-training Data in Transfer Learning» подтверждает важность такого подхода.\n",
    "\n",
    "Благодаря архитектуре НС можно работать «end-to-end» (от входа до выхода) без необходимости вручную прописывать правила сегментации, раскраски и т.д. Это снижает инженерные усилия и повышает воспроизводимость.\n",
    "\n",
    "1.5 Почему не (или хуже) альтернативы\n",
    "\n",
    "Методы ручного цветового переноса или правил (например: «траву зелёной» вручную) не масштабируются, плохо обобщаются на новые сцены/объекты и не учитывают семантику сцены.\n",
    "\n",
    "Гибридные решения (сегментация + правило) требуют раздельной разработки и данных для сегментации, что усложняет архитектуру и увеличивает риск ошибок.\n",
    "\n",
    "Алгоритмы без обучения (например, цветовой перенос) не способны на «понимание» контекста и дают артефакты или неверные цвета. Например в работе Deng (2023) отмечено, что ранее доминировали ручные методы, но они уступают нейросетям по качеству.\n",
    "\n",
    "1.6 Подтверждение через эмпирические исследования\n",
    "\n",
    "В исследовании Deep Learning for the Automatic Colorization of Historical Aerial Images (PMC, 2022) показано, что при применении нейронной сети Hyper-U-Net (вариация U-Net) модель достигла PSNR = 33.508, SSIM = 0.9846 на тесте исторических фото, что является улучшением по сравнению с базовой U-Net (PSNR = 32.9, SSIM = 0.9832) [12].\n",
    "\n",
    "Проект Real-Time User-Guided Image Colorization (Richard Zhang et al. 2017) использовал CNN с большим датасетом (~1 млн изображений), показав, что CNN может функционировать в режиме реального времени и давать качественные результаты.\n",
    "\n",
    "7. Заключение\n",
    "\n",
    "Выбор нейронной сети для системы колоризации является научно оправданным, поскольку:\n",
    "\n",
    "НС позволяют решать сложную, многозначную задачу автоматической раскраски с учётом семантики.\n",
    "\n",
    "Глубокое обучение и генеративные архитектуры обеспечивают качество, близкое к человеческому восприятию.\n",
    "\n",
    "Архитектура НС позволяет адаптироваться (fine-tune), масштабироваться и интегрироваться в веб-сервис.\n",
    "\n",
    "Альтернативные методы либо уступают по качеству, либо сложны в поддержке и не имеют такой гибкости.\n",
    "\n",
    "Эмпирические исследования подтверждают превосходство НС-методов по метрикам и визуальной оценки.\n",
    "\n",
    "Источники выводов:\n",
    "\n",
    "  Anwar S., Tahir M., Li C., Mian A., Khan F. W., Muzafar A. — Image Colorization: A Survey and Dataset, 2020. arXiv+1\n",
    "\n",
    "Huang S., Jin X., Jiang Q., Liu L. — Deep Learning for Image Colorization: Current and Future Prospects, Engineering Applications of Artificial Intelligence, 2022. https://doi.org/10.1016/j.engappai.2022.105006 gwern.net\n",
    "\n",
    "  Dalal H., Dangle A., Mundada R., Shrungare J., Gore S. — Image Colorization Progress: A Review of Deep Learning Techniques for Automation of Colorization, 2021. https://www.warse.org/IJATCSE/static/pdf/file/ijatcse401042021.pdf warse.org\n",
    "\n",
    "  Paper: GAN-Based Image Colorization for Self-Supervised Visual Feature Learning, Sensors, 2022. https://www.mdpi.com/1424-8220/22/4/1599 MDPI\n",
    "\n",
    "  Survey: Computer-aided Colorization State-of-the-Science: A Survey, 2024. https://arxiv.org/html/2410.02288v1 arXiv\n",
    "\n",
    "  Farella E., Malek S., Remondino F. — Colorizing the Past: Deep Learning for the Automatic Colorization of Historical Aerial Images, PMC, 2022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9604844/\n",
    "\n",
    "2. Функциональные требования системы\n",
    "\n",
    "3. Описание  и мотивация выбора компонентов системы.\n",
    "\n",
    "Мы рассмотрели несколько архитектур: традиционные CNN-регрессоры, условные GAN-модели и автоэнкодеры. Однако современная литература по колоризации (Anwar et al., 2020) подчёркивает, что GAN-подходы позволяют генерировать насыщенные, реалистичные цвета и превосходят CNN-регрессию по натуральности результата [1].\n",
    "\n",
    "DeOldify использует:\n",
    "\n",
    "архитектуру U-Net для реконструкции изображения,\n",
    "\n",
    "стратегию NoGAN, позволяющую получать устойчивые результаты без характерных артефактов GAN,\n",
    "\n",
    "perceptual loss, что обеспечивает соответствие восприятия человека.\n",
    "\n",
    "В каталоге сравнений по задаче image colorization на PapersWithCode DeOldify входит в число топовых моделей среди открытых решений [4].\n",
    "\n",
    "В исследовании “Human vs. Objective Evaluation of Colourisation Performance” (2022) показано, что применение перцептуальных функций (LPIPS, SSIM) коррелирует с субъективной оценкой человека [5], что полностью соответствует подходу, применяемому в DeOldify.\n",
    "\n",
    "Мы использовали DeOldify в качестве базовой модели и провели её дообучение (fine-tuning) на датасете COCO 2017. DeOldify предоставляет проверенную архитектуру и предобученные веса, что ускоряет конвергенцию и повышает начальное качество (см. репозиторий DeOldify). COCO 2017 — крупный, разнообразный набор (≈330k изображений), содержащий широкий спектр объектов и контекстов, что делает его подходящим для повышения обобщающей способности модели на реальных фотографиях. Fine-tuning предобученных моделей — общепринятая и научно обоснованная практика для адаптации к целевому распределению данных; современные исследования по fine-tuning и цветизации подтверждают эффективность такого подхода. Валидация улучшений проводится с использованием метрик PSNR/SSIM/LPIPS/FID и с помощью user-study; эти метрики и методики широко применяются в литературе по colorization.\n",
    "\n",
    "Почему выбрана модель DeOldify, а не другие модели колоризации\n",
    "\n",
    "Доказательство выбора:\n",
    "DeOldify — единственная из моделей, массово используемая на коммерческих сервисах колоризации (пример: MyHeritage), что говорит о её зрелости и устойчивости результатов.\n",
    "Субъективные и объективные тесты (PSNR/SSIM/LPIPS) показывают более высокое качество на реальных изображениях, потому что модель «понимает» семантику сцены, а не просто раскрашивает пиксели.\n",
    "\n",
    "Вывод: DeOldify — оптимальный выбор, потому что обеспечивает лучшее качество восстановления цвета и готовые веса позволяют ускорить разработку.\n",
    "\n",
    "При проектировании системы цветизации изображений требовалось подобрать датасет, обеспечивающий широкое разнообразие объектов и сцен, поскольку современные модели колоризации не просто «раскрашивают пиксели», а формируют цвет на основе семантического анализа содержимого изображения (распознают: небо, дерево, лицо, траву). Это подтверждается обзорной статьёй “Image Colorization: A Survey and Dataset” (Anwar et al., 2020), где напрямую указано, что «качество цветизации зависит от семантической информации об объекте, а значит — от разнообразия тренировочного датасета» [1].\n",
    "\n",
    "COCO 2017 содержит более 200 000 изображений и 80 классов объектов, охватывающих людей, животных, транспорт и бытовые сцены. Это соответствует критериям датасета для цветизации: разнообразие, естественные сцены, наличие объектов, требующих разного контекста [2].\n",
    "\n",
    "В работе «The Role of Pre-training Data in Transfer Learning» (2023) отмечено, что «чем ближе распределение данных к целевой задаче, тем выше качество transfer learning» [3].\n",
    "Следовательно, COCO, содержащий реальные сцены, а не только классы изображений без контекста, является оптимальным для цветизации.\n",
    "\n",
    "Почему дообучение на COCO 2017, а не ImageNet / CelebA / Places365\n",
    "\n",
    "Доказательство выбора:\n",
    "Задача колоризации требует правильного понимания контекста. Если модель видит дерево — оно должно стать зелёным, небо — синим, кожа — телесной.\n",
    "COCO — единственный датасет, включающий и объекты, и сцены, и людей одновременно. Поэтому дообучение DeOldify на COCO:\n",
    "\n",
    "улучшает семантические привязки цвета;\n",
    "\n",
    "снижает ошибки на реальных фото (по сравнению с ImageNet, где мало сцен).\n",
    "\n",
    "Вывод: COCO обеспечивает наилучшее покрытие реальных сценариев использования и улучшает качество цветопередачи.\n",
    "\n",
    "Согласно обзору “A Survey on Fine-Tuning Pretrained Models in Computer Vision” (2023), fine-tuning позволяет:\n",
    "\n",
    "уменьшить размер требуемого датасета,\n",
    "\n",
    "сократить время обучения,\n",
    "\n",
    "улучшить итоговое качество модели [6].\n",
    "\n",
    "В официальном руководстве TensorFlow указано, что «предобученная модель уже извлекла полезные признаки, поэтому обучение начинается существенно быстрее» [7].\n",
    "\n",
    "3.1 Выбор языков программирования и фреймворков для разработки информационной системы. \n",
    "\n",
    " Python (язык)\n",
    "\n",
    "Причина: Доминирующий язык в ML и научных исследованиях; богатая экосистема (NumPy, SciPy, OpenCV, PyTorch, HuggingFace и т.д.), быстрая разработка и прочная поддержка сообществом [10][5]. Это обеспечивает доступ к репозиториям DeOldify и инструментам для обработки изображений.\n",
    "\n",
    "Почему Python как язык разработки\n",
    "\n",
    "Доказательство выбора:\n",
    "90% научных публикаций и фреймворков машинного обучения — Python.\n",
    "Все фреймворки, необходимые для DeOldify, доступны только на Python.\n",
    "\n",
    "Вывод: выбор языка фактически определён самим ML-стеком. \n",
    "\n",
    "PyTorch (фреймворк)\n",
    "\n",
    "Причина: DeOldify реализован на PyTorch → прямой перенос/дообучение без переписывания модели; PyTorch обеспечивает динамический граф, удобство экспериментов и зрелую инфраструктуру для production (TorchScript, TorchServe) [9][11]. PyTorch широко рекомендован для R&D работ в CV [11].\n",
    "\n",
    "Почему PyTorch, а не TensorFlow/Keras\n",
    "\n",
    "Доказательство выбора:\n",
    "DeOldify официально написан на PyTorch → перенос на TensorFlow потребовал бы переписывания архитектуры.\n",
    "\n",
    "Вывод: PyTorch выбирается не только как лучший фреймворк для R&D, но и как единственно совместимый с DeOldify.\n",
    "\n",
    "Backend — FastAPI\n",
    "\n",
    "Причина: FastAPI обеспечивает асинхронную обработку запросов, автогенерацию OpenAPI (Swagger), и в бенчмарках показывает высокую пропускную способность по сравнению с традиционными Flask-подходами — важно для параллельных запросов инференса и удобства разработки API [11][17].\n",
    "\n",
    "Почему FastAPI (backend), а не Flask / Django\n",
    "\n",
    "Вывод: FastAPI создан для быстрого API + ML inference.\n",
    "\n",
    "Backend — FastAPI\n",
    "\n",
    "Причина: FastAPI обеспечивает асинхронную обработку запросов, автогенерацию OpenAPI (Swagger), и в бенчмарках показывает высокую пропускную способность по сравнению с традиционными Flask-подходами — важно для параллельных запросов инференса и удобства разработки API [11][17].\n",
    "\n",
    "Почему React (frontend), а не Vue / Angular\n",
    "\n",
    "Вывод: React — оптимальный выбор для быстрой разработки UI и интеграции с API.\n",
    "\n",
    "Почему Docker + Redis\n",
    "\n",
    "Причина: Контейнеризация обеспечивает воспроизводимость окружения (версии CUDA, Python, библиотек) и облегчает развёртывание и тестирование; использование Docker для репродукции научных экспериментов и деплоя широко рекомендовано в академии и индустрии [13][19].\n",
    "\n",
    "Причина: Redis — in-memory store, подходящий для cache-aside паттерна (кеширование результатов инференса по хэшу изображения), значительно снижает латентность повторных запросов и нагрузку на GPU. Redis официально позиционирует себя как решение для кеширования и RAG/AI-приложений [14].\n",
    "\n",
    "Без Redis система бы каждый раз запускала модель → медленно.\n",
    "\n",
    "Каждый компонент архитектуры выбран не случайно, а на основе сравнительного анализа альтернатив. DeOldify + COCO дают наилучшее качество колоризации среди доступных моделей и датасетов, PyTorch + Python обеспечивают возможность дообучения, FastAPI и React — лёгкость интеграции и высокую скорость. Docker и Redis позволяют сделать систему масштабируемой и стабильной в нагрузке. \n",
    "\n",
    "Рисунок 2 – Архитектура информационной системы ColorAIze\n",
    "\n",
    "ADR (Architectural Decision Record) — краткие решения и мотивация\n",
    "\n",
    "ADR-01: Модель — DeOldify (арт. причина: проверенная GAN/NoGAN архитектура, предобученные веса, качество). Ссылки: [1][3][4].\n",
    "ADR-02: Дообучение на COCO2017 (обоснование: семантическое покрытие сцен и объектов — критично для корректной цветизации) — [2][3][6].\n",
    "ADR-03: Язык/фреймворк — Python + PyTorch (прямо совместимы с DeOldify; удобство R&D) — [9][10].\n",
    "ADR-04: Backend/API — FastAPI (асинхронность, автодокументация, производительность) — [11].\n",
    "ADR-05: Frontend — React (компонентная разработка, широкая поддержка) — [12].\n",
    "ADR-06: Развёртывание — Docker, кеш — Redis (репродуцируемость и снизить нагрузку на ML-слой) — [13][14].\n",
    "\n",
    "Рисунок 3 Стек технологий системы olorAlze\n",
    "\n",
    "Заключение \n",
    "\n",
    "В рамках практической работы разработана информационная интеллектуальная система для автоматической колоризации изображений. В качестве базовой модели выбрана DeOldify, использующая GAN/NoGAN-подход и перцептуальные loss-функции, что обеспечивает высокое перцептуальное качество (см. репозиторий DeOldify) [1] и согласуется с современными обзорами по colorization [3]. Для повышения обобщающей способности проведено controlled fine-tuning на датасете COCO2017 (≈328k изображений), обладающем необходимым семантическим разнообразием объектов и сцен, критичным для корректной цветовой предсказуемости [2][6]. Методика fine-tuning разработана с учётом рисков и рекомендаций по LP-FT (linear probe → full fine-tune) и мониторинга OOD поведения [7][8]. Технический стек (Python, PyTorch, FastAPI, React, Docker, Redis) выбран на основании совместимости с ML-инструментами, производительности и требований к воспроизводимости и масштабируемости [9][10][11][12][13][14].\n",
    "\n",
    "Источники по колоризации, нейросетям и архитектурным решениям\n",
    "\n",
    "DeOldify — GitHub репозиторий\n",
    "https://github.com/jantic/DeOldify\n",
    "\n",
    "Lin, T.-Y. et al. — Microsoft COCO: Common Objects in Context, ECCV 2014 / arXiv:1405.0312\n",
    "https://arxiv.org/abs/1405.0312\n",
    "\n",
    "Anwar, S. et al. — Image Colorization: A Survey and Dataset, 2020\n",
    "https://arxiv.org/abs/2008.10774\n",
    "\n",
    "PapersWithCode — Image Colorization (leaderboards, datasets, SOTA модели)\n",
    "https://paperswithcode.com/task/image-colorization\n",
    "\n",
    "Zhang, R. et al. — The Unreasonable Effectiveness of Deep Features as a Perceptual Metric (LPIPS), CVPR 2018\n",
    "https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf\n",
    "\n",
    "Entezari, R. — The Role of Pre-training Data in Transfer Learning, 2023\n",
    "https://arxiv.org/abs/2302.13602\n",
    "\n",
    "Kumar, A. et al. — Fine-Tuning Can Distort Pretrained Features and Underperform Out-of-Distribution, 2022\n",
    "https://arxiv.org/abs/2202.10054\n",
    "\n",
    "Survey — A Survey on Fine-Tuning Pretrained Model in Computer Vision, 2023\n",
    "https://www.researchgate.net/publication/371289816_A_Survey_on_Fine_Tuning_Pretrained_Model_in_Computer_Vision\n",
    "\n",
    "Human vs. Objective Evaluation of Colourisation Performance, 2022\n",
    "https://arxiv.org/abs/2204.05200\n",
    "\n",
    "Источники по инструментам и техническому стеку (Python / PyTorch / FastAPI / React / Docker / Redis)\n",
    "\n",
    "PyTorch — официальный сайт и документация\n",
    "https://pytorch.org/\n",
    "\n",
    "TensorFlow — Transfer Learning Guide (официальная документация)\n",
    "https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "\n",
    "Python in AI / ML — survey\n",
    "Leveraging Python in AI and Machine Learning (survey, 2024)\n",
    "https://www.researchgate.net/publication/388852036_Leveraging_Python_in_AI_and_Machine_Learning_A_Survey_of_Techniques_and_Educational_Approaches_in_Software_Engineering\n",
    "\n",
    "FastAPI — сравнительный обзор (FastAPI vs Flask)\n",
    "https://www.codecademy.com/article/fastapi-vs-flask-key-differences-performance-and-use-cases\n",
    "\n",
    "React — официальная документация\n",
    "https://react.dev/learn\n",
    "\n",
    "Docker — для воспроизводимых научных исследований\n",
    "Boettiger, Carl. An Introduction to Docker for Reproducible Research, 2015\n",
    "https://doi.org/10.1145/2889160.2891057\n",
    "\n",
    "Redis — официальная документация / caching solutions\n",
    "https://redis.io/solutions/caching\n",
    "\n",
    "Дополнительные источники, упомянутые в ранних частях\n",
    "\n",
    "Google AI / TensorFlow — Transfer learning best practices\n",
    "(дублирует пункт 11, оставлено для полноты)\n",
    "https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "\n",
    "pix2pix / CycleGAN original papers (для сравнения моделей)\n",
    "pix2pix: Image-to-Image Translation with Conditional Adversarial Networks\n",
    "https://arxiv.org/abs/1611.07004\n",
    "\n",
    "CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n",
    "https://arxiv.org/abs/1703.10593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5192d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Пример запроса (формат реального API)\n",
    "upload_request = {\n",
    "    \"endpoint\": \"POST /api/images\",\n",
    "    \"headers\": {\"X-Session-ID\": \"a1b2c3...\"},\n",
    "    \"body\": \"<binary image/jpeg>\",\n",
    "}\n",
    "\n",
    "# Пример ответа (структура реального контракта)\n",
    "upload_response = {\n",
    "    \"id\": 123,\n",
    "    \"status\": \"pending\",\n",
    "    \"createdAt\": \"2025-12-20T12:00:00Z\"\n",
    "}\n",
    "\n",
    "pprint(upload_request)\n",
    "pprint(upload_response)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
